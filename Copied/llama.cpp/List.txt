  Core Foundation (Must Have)

  1. ✅ include/llama.h - Public API (line: 1040-1070) // 850 - 1270
  2. ✅ src/llama.cpp - Main implementation
  3. ggml/include/ggml.h - Tensor library API
  4. ggml/src/ggml.c - Core tensor operations

  Model Architecture

  5. src/llama-model.h/cpp - Model structure & loading
  6. src/llama-arch.h/cpp - Architecture registry (GPT, Llama, etc.)
  7. src/llama-hparams.h/cpp - Model hyperparameters

  Context & Inference

  8. src/llama-context.h/cpp - Inference context management
  9. src/llama-batch.h/cpp - Batch processing (you've seen this!)
  10. src/llama-sampling.h/cpp - Token sampling strategies

  Memory Management

  11. src/llama-kv-cache-unified.h/cpp - KV cache (critical for inference)
  12. ggml/include/ggml-backend.h - Backend abstraction
  13. ggml/src/ggml-backend.cpp - Backend implementation

  Tokenization

  14. src/llama-vocab.h/cpp - Vocabulary & tokenization
  15. src/llama-grammar.h/cpp - Grammar constraints

  Quantization

  16. ggml/src/ggml-quants.h/c - Quantization formats (Q4_0, Q8_0, etc.)

  Examples (See it in Action)

  17: src/llama-io.h/cpp - File I/O operations
  18. examples/simple/simple.cpp - Minimal usage example

  Common Utilities

  19. common/common.h/cpp - Shared utilities
  20. common/sampling.h/cpp - High-level sampling helpers

  Model Loading

  21. src/llama-model-loader.h/cpp - GGUF loading
  22. ggml/include/gguf.h - GGUF format definitions

  Implementation Details

  23. src/llama-impl.h/cpp - Internal helpers
  24. ggml/src/ggml-impl.h - Low-level tensor helpers

  Bonus

  25. tools/server/server.cpp - HTTP API server
  26. src/llama-quant.h/cpp - Quantization utilities
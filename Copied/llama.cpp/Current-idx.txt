

  5. Tokenization Core (Lines ~1200-1230)

  // Text to tokens - returns negative on failure!
  LLAMA_API int32_t llama_tokenize(
      const struct llama_vocab * vocab,
                    const char * text,
                       int32_t   text_len,
                   llama_token * tokens,
                       int32_t   n_tokens_max,
                          bool   add_special,     // Add BOS/EOS
                          bool   parse_special);  // Parse control tokens

  // Tokens back to text
  LLAMA_API int32_t llama_detokenize(
      const struct llama_vocab * vocab,
             const llama_token * tokens,
                       int32_t   n_tokens,
                          char * text,
                       int32_t   text_len_max,
                          bool   remove_special,
                          bool   unparse_special);

  Why learn: Core tokenization with important flags. Negative returns indicate buffer too small.

  6. Sampler Interface Pattern (Lines ~1340-1370)

  // Sampler interface - strategy pattern for token selection
  struct llama_sampler_i {
      const char *           (*name)  (const struct llama_sampler * smpl);
      void                   (*accept)(      struct llama_sampler * smpl, llama_token token);
      void                   (*apply) (      struct llama_sampler * smpl, llama_token_data_array * cur_p);
      void                   (*reset) (      struct llama_sampler * smpl);
      struct llama_sampler * (*clone) (const struct llama_sampler * smpl);
      void                   (*free)  (      struct llama_sampler * smpl);
  };

  struct llama_sampler {
      const struct llama_sampler_i * iface;  // vtable pattern
      llama_sampler_context_t        ctx;
  };

  Why learn: Beautiful C implementation of OOP/strategy pattern. Shows how to make extensible APIs in C.

  7. Sampler Chain Example (Lines ~1400-1420)

  // Chain samplers together for complex sampling strategies
  LLAMA_API struct llama_sampler * llama_sampler_chain_init(struct llama_sampler_chain_params params);

  // Takes ownership!
  LLAMA_API void llama_sampler_chain_add(struct llama_sampler * chain, struct llama_sampler * smpl);

  // Available samplers
  LLAMA_API struct llama_sampler * llama_sampler_init_top_k(int32_t k);
  LLAMA_API struct llama_sampler * llama_sampler_init_top_p(float p, size_t min_keep);
  LLAMA_API struct llama_sampler * llama_sampler_init_temp(float t);

  Why learn: Shows composable sampling pipeline. Chain takes ownership = it will free the samplers.

  8. Chat Template Application (Lines ~1250-1270)

  // Apply chat template (like ChatML, Llama2, etc.)
  LLAMA_API int32_t llama_chat_apply_template(
                          const char * tmpl,      // Custom Jinja template
     const struct llama_chat_message * chat,     // Array of messages
                              size_t   n_msg,
                                bool   add_ass,   // Add assistant prefix
                                char * buf,
                             int32_t   length);

  Why learn: Critical for chat models - converts conversation to model's expected format.

  Key Learning Points:

  1. Return codes matter: Negative = error, positive = warning
  2. Ownership semantics: Chain "takes ownership" = it frees the resource
  3. Negative indices: -1 means "last item" (Python-style)
  4. Special tokens: Different models use different control tokens
  5. Interface pattern: C structs with function pointers = OOP in C
  6. Buffer patterns: Functions return required size when buffer too small

  This section shows the actual inference pipeline - from tokens to logits to sampling!